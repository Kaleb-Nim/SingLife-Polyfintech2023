{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with promopts\n",
    "\n",
    "Assumed Pinecone db is already created and populated with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\pinecone\\index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH_TO_ENV:  c:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\custom_llm\\.env\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "import openai\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Langchain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.callbacks import wandb_tracing_enabled\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Optional\n",
    "from langchain.chains import SimpleSequentialChain ,SequentialChain\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.schema import HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "# wandb\n",
    "import wandb \n",
    "\n",
    "# Import singlife from utils \n",
    "from utils.singlife import Singlife"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "* Load all the API keys from Pinecone and OpenAI\n",
    "* Load the Pinecone client\n",
    "* Set-up wandb tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables from the .env file\n",
    "load_dotenv('../Sn33k/.env')\n",
    "\n",
    "# Access the variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "PINECONE_ENVIRONMENT= os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# wandb\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'test_llm_pinecone.ipynb'\n",
    "# here we are configuring the wandb project name\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Singlife\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WANDB setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaleb-nim\u001b[0m (\u001b[33mshiok\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb_config = {\"project\": \"Singlife\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/shiok/Singlife/runs/7ftek8vp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.tracers import WandbTracer \n",
    "Tracer = WandbTracer(wandb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],\n",
       " 'kwargs': {'model_name': 'gpt-3.5-turbo-0613',\n",
       "  'temperature': 0.0,\n",
       "  'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt-3.5-turbo-0613\"\n",
    "temperature = 0.0\n",
    "llm_qa = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "llm_qa.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],\n",
       " 'kwargs': {'model_name': 'gpt-3.5-turbo-0613',\n",
       "  'temperature': 0.0,\n",
       "  'model_kwargs': {'functions': [{'name': 'output_formatter',\n",
       "     'description': 'Output formatter. Should always be used to format your response to the user.',\n",
       "     'parameters': {'title': 'generate_video_script',\n",
       "      'description': 'Generates 15-30sec video script based on custom knowledge base. Two components 1.Scene assets descriptions 2.Subtitle script',\n",
       "      'type': 'object',\n",
       "      'properties': {'list_of_video_chunk': {'type': 'array',\n",
       "        'description': 'List of video_chunk to be included in the video, one video chunk should last 3-5 seconds and includes: 1. Scene  2. Subtitle',\n",
       "        'items': {'type': 'object'},\n",
       "        'properties': {'scene': {'type': 'string',\n",
       "          'description': 'Scene description for video should be visual and general'},\n",
       "         'subtitles': {'type': 'string',\n",
       "          'description': 'Funny and sarcastic video subtitles script for video'}},\n",
       "        'required': ['scene', 'subtitles']}}}}],\n",
       "   'function_call': {'name': 'output_formatter'}},\n",
       "  'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt-3.5-turbo-0613\"\n",
    "temperature = 0.0\n",
    "llm_video_script = ChatOpenAI(model_name=model_name, temperature=temperature, model_kwargs= {\"functions\":[\n",
    "    {\n",
    "      \"name\": \"output_formatter\",\n",
    "      \"description\": \"Output formatter. Should always be used to format your response to the user.\",\n",
    "      \"parameters\": {\n",
    "        \"title\": \"generate_video_script\",\n",
    "        \"description\": \"Generates 15-30sec video script based on custom knowledge base. Two components 1.Scene assets descriptions 2.Subtitle script\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"list_of_video_chunk\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of video_chunk to be included in the video, one video chunk should last 3-5 seconds and includes: 1. Scene  2. Subtitle\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\"\n",
    "            },\n",
    "            \"properties\": {\n",
    "              \"scene\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Scene description for video should be visual and general\"\n",
    "              },\n",
    "              \"subtitles\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Funny and sarcastic video subtitles script for video\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"scene\", \"subtitles\"]\n",
    "          }\n",
    "        },\n",
    "      }\n",
    "    }\n",
    "],\"function_call\":{\"name\":\"output_formatter\"}})\n",
    "llm_video_script.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstore Setup ( Pinecone )\n",
    "\n",
    "Pinecone integration with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Pinecone client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_description:  IndexDescription(name='singlife', metric='cosine', replicas=1, dimension=1536.0, shards=1, pods=1, pod_type='p1', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')\n",
      "index_stats_response:  {'dimension': 1536,\n",
      " 'index_fullness': 0.1,\n",
      " 'namespaces': {'': {'vector_count': 3488}},\n",
      " 'total_vector_count': 3488}\n"
     ]
    }
   ],
   "source": [
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENVIRONMENT,  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"singlife\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "# if you already have an index, you can load it like this\n",
    "docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "# List all indexes information\n",
    "index_description = pinecone.describe_index(index_name)\n",
    "print('index_description: ', index_description)\n",
    "\n",
    "index = pinecone.Index(index_name) \n",
    "index_stats_response = index.describe_index_stats()\n",
    "print('index_stats_response: ', index_stats_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Pinecone(index, embeddings.embed_query, \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling\n",
    "\n",
    "> describe functions to gpt-4-0613 and gpt-3.5-turbo-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions\n",
    "\n",
    "\n",
    "models have been fine-tuned to both detect when a function needs to be called (depending on the userâ€™s input) and to respond with JSON that adheres to the function signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions are specified with the following fields:\n",
    "\n",
    "* Name: The name of the function.\n",
    "* Description: A description of what the function does. The model will use this to decide when to call the function.\n",
    "* Parameters: The parameters object contains all of the input fields the function requires. These inputs can be of the following types: String, Number, Boolean, Object, Null, AnyOf. Refer to the API reference docs for details.\n",
    "* Required: Which of the parameters are required to make a query. The rest will be treated as optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create QAretriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_qa,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(memory=None, callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, combine_documents_chain=RefineDocumentsChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, input_key='input_documents', output_key='output_text', initial_llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['question', 'context_str'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context_str'], output_parser=None, partial_variables={}, template='Context information is below. \\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer any questions', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), refine_llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['question', 'context_str', 'existing_answer'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['existing_answer'], output_parser=None, partial_variables={}, template='{existing_answer}', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context_str'], output_parser=None, partial_variables={}, template=\"We have the opportunity to refine the existing answer(only if needed) with some more context below.\\n------------\\n{context_str}\\n------------\\nGiven the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\", template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), document_variable_name='context_str', initial_response_name='existing_answer', document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), return_intermediate_steps=False), input_key='query', output_key='result', return_source_documents=False, retriever=VectorStoreRetriever(tags=None, metadata=None, vectorstore=<langchain.vectorstores.pinecone.Pinecone object at 0x00000169ED87B130>, search_type='similarity', search_kwargs={}))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?',\n",
       " 'result': \"Based on the provided context, it appears that the Singlife Corporate Travel Insurance policy is not suitable for a personal ski trip with your family. The policy is specifically designed for corporate travel purposes and may not provide the necessary coverage for leisure activities such as skiing.\\n\\nFor a ski trip with your family, it is recommended to consider a travel insurance policy that specifically covers recreational activities like skiing. Look for policies that include coverage for medical expenses related to skiing accidents, emergency medical evacuation, trip cancellation or interruption, and lost or damaged ski equipment.\\n\\nIt is important to review the policy details, including coverage limits, exclusions, and any specific requirements or conditions related to skiing activities. Consider exploring travel insurance options from providers that specialize in leisure or sports-related activities to ensure you have the appropriate coverage for your family's ski trip.\\n\\nRemember to disclose any pre-existing medical conditions and discuss with the insurance provider to ensure that you have the necessary coverage for your specific needs during the ski trip.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\"\n",
    "response = qa_chain(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template for Video chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['result', 'query'], output_parser=None, partial_variables={}, template='Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \\n    Custom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\nReturn the generated video script in the style/format: Funny and sarcastic', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template for video script generation\n",
    "video_prompt = PromptTemplate(\n",
    "    template=\"\"\"Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \n",
    "    Custom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\nReturn the generated video script in the style/format: Funny and sarcastic\"\"\",\n",
    "    input_variables= [\"result\", \"query\"]\n",
    ")\n",
    "video_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['result', 'query'], output_parser=None, partial_variables={}, template='Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \\n    Custom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\nReturn the generated video script in the style/format: Funny and sarcastic', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_prompt_style =\"Funny and sarcastic\"\n",
    "video_prompt = PromptTemplate(\n",
    "    template=\"\"\"Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \n",
    "    Custom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\nReturn the generated video script in the style/format: \"\"\"+video_prompt_style,\n",
    "    input_variables= [\"result\", \"query\"]\n",
    ")\n",
    "video_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the video Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "        \"title\": \"generate_video_script\",\n",
    "        \"description\": \"Generates 15-30sec video script based on custom knowledge base. Two components 1.Scene descriptions 2.Subtitle script\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"list_of_video_chunk\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of video_chunk to be included in the video, one video chunk should last 3-5 seconds and is a dictionary with keys: 1. Scene  2. Subtitle\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\"\n",
    "            },\n",
    "            \"properties\": {\n",
    "              \"scene\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Scene description for video should be visual and general\"\n",
    "              },\n",
    "              \"subtitles\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Funny and sarcastic video subtitles script for video\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"scene\", \"subtitles\"]\n",
    "          }\n",
    "        },\"required\": [\"list_of_video_chunk\"],\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_chain2 = create_structured_output_chain(json_schema, llm_qa, video_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain for video script generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(chains=[qa_chain, video_chain2],input_variables=[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:av81yds5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-cosmos-6</strong> at: <a href='https://wandb.ai/shiok/Singlife/runs/av81yds5' target=\"_blank\">https://wandb.ai/shiok/Singlife/runs/av81yds5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230719_142852-av81yds5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:av81yds5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\custom_llm\\wandb\\run-20230719_170314-ws23vjbj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shiok/Singlife/runs/ws23vjbj' target=\"_blank\">twilight-snowflake-7</a></strong> to <a href='https://wandb.ai/shiok/Singlife' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shiok/Singlife' target=\"_blank\">https://wandb.ai/shiok/Singlife</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shiok/Singlife/runs/ws23vjbj' target=\"_blank\">https://wandb.ai/shiok/Singlife/runs/ws23vjbj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/shiok/Singlife/runs/ws23vjbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1a1952a7730>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Singlife\", config=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGoal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \n",
      "    Custom knowledge base:Based on the provided context, it appears that the Singlife Corporate Travel Insurance policy is not suitable for a personal ski trip with your family. The policy is specifically designed for corporate travel purposes and may not provide the necessary coverage for leisure activities such as skiing.\n",
      "\n",
      "For your ski trip to Japan, it is recommended to seek out a travel insurance policy that specifically covers recreational activities like skiing. Look for a policy that includes coverage for medical expenses related to skiing accidents, emergency medical evacuation, trip cancellation or interruption, and loss or damage to ski equipment.\n",
      "\n",
      "It is important to carefully review the policy details, including coverage limits, exclusions, and any additional options or endorsements that may be available to enhance your coverage specifically for skiing activities. Consider exploring travel insurance options from providers that specialize in leisure or sports-related activities to ensure you have the appropriate coverage for your ski trip with your family.\n",
      "\n",
      "Remember to disclose any pre-existing medical conditions to the insurance provider and inquire about coverage for those conditions as well. Having the right travel insurance coverage will give you peace of mind and protect you and your family during your ski trip in Japan.\n",
      "\n",
      "Using the above information, generate a video script that addresses this user query:\n",
      "\n",
      "\"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\".\n",
      "Return the generated video script in the style/format: Funny and sarcastic\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'scene_assets_description': \"The video starts with a family packing their bags for their ski trip to Japan. They are excited and energetic, wearing winter clothes and holding ski equipment. The scene transitions to a comical animation of a corporate office, with people in suits and ties. The animation shows a sign that says 'Singlife Corporate Travel Insurance'. The scene quickly changes to a beautiful snowy mountain in Japan, with skiers enjoying their time on the slopes. The video ends with the family happily skiing together and a text overlay that says 'Get the right coverage for your ski trip!'\",\n",
       " 'subtitle_script': \"Subtitle 1: Family: We're going skiing in Japan!\\nSubtitle 2: Narrator: But wait, not all travel insurance policies are created equal.\\nSubtitle 3: Narrator: The Singlife Corporate Travel Insurance policy may not be suitable for your family ski trip.\\nSubtitle 4: Narrator: You need a policy that covers recreational activities like skiing.\\nSubtitle 5: Narrator: Look for coverage for medical expenses, emergency evacuation, trip cancellation, and ski equipment.\\nSubtitle 6: Narrator: Don't forget to review the policy details and exclusions.\\nSubtitle 7: Narrator: And hey, disclose any pre-existing medical conditions too!\\nSubtitle 8: Narrator: Get the right coverage for your ski trip and have a blast in Japan!\\nSubtitle 9: Text Overlay: Get the right coverage for your ski trip!\",\n",
       " 'video_length': '15-30 seconds'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = overall_chain.run(query=query,callbacks=[Tracer])\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the experiment begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatOpenAI' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[253], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# wandb trace\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m wandb_tracing_enabled():\n\u001b[1;32m---> 17\u001b[0m     first_response \u001b[39m=\u001b[39m llm_video_script\u001b[39m.\u001b[39;49mrun(result\u001b[39m=\u001b[39muser_request, query\u001b[39m=\u001b[39mquery)\n\u001b[0;32m     19\u001b[0m first_response\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChatOpenAI' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "user_request = \"\"\"Based on the Singlife Corporate Travel Insurance policy, you should consider the following coverage for your ski trip to Japan:\n",
    "1. Accidental death, permanent disablement and burns benefit: This provides coverage in case of accidental injury during your trip.\n",
    "2. Medical and medical evacuation: This is crucial for a ski trip as it covers any medical emergencies or injuries that may occur, including the cost of evacuation if necessary.\n",
    "3. Trip cancellation: If there's a last-minute cancellation, you can receive coverage for non-refundable deposits or unused travel and accommodation costs.\n",
    "4. Full terrorism cover: This offers a lump-sum payout if an unfortunate event occurs.\n",
    "5. Delayed departure, missed departure or connection: This covers any additional expenses incurred due to delayed or missed flights.\n",
    "6. Loss or damage of baggage and personal belongings: This provides coverage for lost or damaged personal items during your trip.\n",
    "7. Rental vehicle excess: If you plan to rent a vehicle during your trip, this coverage can be beneficial.\n",
    "8. COVID-19 Coverage: The policy covers trip interruptions or cancellations due to COVID-19 and covers the medical treatment if you contract COVID-19 during or after the trip.\n",
    "Remember to choose a plan that best suits your family's needs. The Elite Plan offers comprehensive coverage for frequent travellers, while the Classic Plan covers the basics.\n",
    "\n",
    "Using the above information, generate a video script that addresses this user query:I am travelling to Japan for a ski \n",
    "trip with my family next week. What kind of travel insurance coverage do we need? In the format of sarcastic and funny.\"\"\"\n",
    "\n",
    "# wandb trace\n",
    "with wandb_tracing_enabled():\n",
    "    first_response = llm_video_script\n",
    "\n",
    "first_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation testing ( Delete kalate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions.openapi import get_openapi_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequentialChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, chains=[LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['query'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], output_parser=None, partial_variables={}, template=\"Use the provided API's to respond to this user query:\\n\\n{query}\", template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='function', output_parser=JsonOutputFunctionsParser(args_only=False), return_final_only=True, llm_kwargs={'functions': [{'name': 'productsUsingGET', 'description': 'API for fetching Klarna product information', 'parameters': {'type': 'object', 'properties': {'params': {'type': 'object', 'properties': {'countryCode': {'type': 'string', 'description': 'ISO 3166 country code with 2 characters based on the user location. Currently, only US, GB, DE, SE and DK are supported.'}, 'q': {'type': 'string', 'description': \"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. If the user speaks another language than English, translate their request into English (example: translate fia med knuff to ludo board game)!\"}, 'size': {'type': 'integer', 'description': 'number of products returned'}, 'min_price': {'type': 'integer', 'description': \"(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\"}, 'max_price': {'type': 'integer', 'description': \"(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\"}}, 'required': ['countryCode', 'q']}}}}]}), SimpleRequestChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, request_method=<function get_openapi_chain.<locals>.<lambda> at 0x000001A192683AC0>, output_key='response', input_key='function')], input_variables=['query'], output_variables=['response'], return_all=False)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = get_openapi_chain(\n",
    "    \"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\"\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'products': [{'name': \"Levi's Men's Classic Button-Down Shirt, Medium\",\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3202043708/Clothing/Levi-s-Men-s-Classic-Button-Down-Shirt-Medium/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$15.99',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Blue,Purple,Black,Orange,Green',\n",
       "    'Size:S,XL,L,M,XXL']},\n",
       "  {'name': 'Cubavera Four Pocket Guayabera Shirt',\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3202055522/Clothing/Cubavera-Four-Pocket-Guayabera-Shirt/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$24.99',\n",
       "   'attributes': ['Material:Polyester,Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Red,White,Blue,Black',\n",
       "    'Properties:Pockets',\n",
       "    'Pattern:Solid Color',\n",
       "    'Size:S,XL,L,M,XXL']},\n",
       "  {'name': 'Short Sleeve Cotton Plaid Shirt',\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201940921/Clothing/Short-Sleeve-Cotton-Plaid-Shirt/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$16.99',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Gray,Blue,Green',\n",
       "    'Size:XL,L']},\n",
       "  {'name': 'Vineyard Vines Gingham On-The-Go brrr Classic Fit Shirt Crystal',\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201938510/Clothing/Vineyard-Vines-Gingham-On-The-Go-brrr-Classic-Fit-Shirt-Crystal/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$89.60',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Blue',\n",
       "    'Size:XL,XS,L,M']},\n",
       "  {'name': \"Carhartt Men's Loose Fit Midweight Short Sleeve Plaid Shirt\",\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201826024/Clothing/Carhartt-Men-s-Loose-Fit-Midweight-Short-Sleeve-Plaid-Shirt/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$17.49',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Red,Brown,Blue,Green',\n",
       "    'Properties:Pockets',\n",
       "    'Pattern:Checkered',\n",
       "    'Size:S,XL,L,M']}]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What are some options for a men's large blue button down shirt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Skipping trace saving - unable to safely convert LangChain Run into W&B Trace due to: 'NoneType' object has no attribute 'get'\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Incorrect API key provided: sk-oWbZ7***************************************X6ta. You can find your API key at https://platform.openai.com/account/api-keys.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 53\u001b[0m\n\u001b[0;32m      1\u001b[0m llm  \u001b[39m=\u001b[39m ChatOpenAI(model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-4-0613\u001b[39m\u001b[39m\"\u001b[39m, temperature\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\u001b[39mExample Update summary:\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[39mFully Custom Trained BERTopic Model for verification of disruption events from News Articles\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39m    * Webscraped 2.5k news articles from 4 different sources + manually labeled test set 375 \u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mData Analyst on Mirxes internal data\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[39mShow the mapping of the gastroclear graph\u001b[39m\u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m---> 53\u001b[0m result \u001b[39m=\u001b[39m llm\u001b[39m.\u001b[39;49mpredict(query)\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chat_models\\base.py:385\u001b[0m, in \u001b[0;36mBaseChatModel.predict\u001b[1;34m(self, text, stop, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     _stop \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(stop)\n\u001b[1;32m--> 385\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m([HumanMessage(content\u001b[39m=\u001b[39mtext)], stop\u001b[39m=\u001b[39m_stop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chat_models\\base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.__call__\u001b[1;34m(self, messages, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[0;32m    343\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    344\u001b[0m     messages: List[BaseMessage],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[0;32m    348\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BaseMessage:\n\u001b[1;32m--> 349\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[0;32m    350\u001b[0m         [messages], stop\u001b[39m=\u001b[39mstop, callbacks\u001b[39m=\u001b[39mcallbacks, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    351\u001b[0m     )\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m    352\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(generation, ChatGeneration):\n\u001b[0;32m    353\u001b[0m         \u001b[39mreturn\u001b[39;00m generation\u001b[39m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chat_models\\base.py:125\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n\u001b[0;32m    124\u001b[0m             run_managers[i]\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 125\u001b[0m         \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    126\u001b[0m flattened_outputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    127\u001b[0m     LLMResult(generations\u001b[39m=\u001b[39m[res\u001b[39m.\u001b[39mgenerations], llm_output\u001b[39m=\u001b[39mres\u001b[39m.\u001b[39mllm_output)\n\u001b[0;32m    128\u001b[0m     \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results\n\u001b[0;32m    129\u001b[0m ]\n\u001b[0;32m    130\u001b[0m llm_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_combine_llm_outputs([res\u001b[39m.\u001b[39mllm_output \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chat_models\\base.py:115\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m i, m \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(messages):\n\u001b[0;32m    113\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m         results\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 115\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    116\u001b[0m                 m,\n\u001b[0;32m    117\u001b[0m                 stop\u001b[39m=\u001b[39mstop,\n\u001b[0;32m    118\u001b[0m                 run_manager\u001b[39m=\u001b[39mrun_managers[i] \u001b[39mif\u001b[39;00m run_managers \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    119\u001b[0m                 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    120\u001b[0m             )\n\u001b[0;32m    121\u001b[0m         )\n\u001b[0;32m    122\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chat_models\\base.py:262\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    259\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m     )\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m new_arg_supported:\n\u001b[1;32m--> 262\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(\n\u001b[0;32m    263\u001b[0m         messages, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    264\u001b[0m     )\n\u001b[0;32m    265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chat_models\\openai.py:371\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    363\u001b[0m     message \u001b[39m=\u001b[39m _convert_dict_to_message(\n\u001b[0;32m    364\u001b[0m         {\n\u001b[0;32m    365\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: inner_completion,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         }\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m ChatResult(generations\u001b[39m=\u001b[39m[ChatGeneration(message\u001b[39m=\u001b[39mmessage)])\n\u001b[1;32m--> 371\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompletion_with_retry(messages\u001b[39m=\u001b[39mmessage_dicts, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m    372\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chat_models\\openai.py:319\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    317\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 319\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(f, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\langchain\\chat_models\\openai.py:317\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 317\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\openai\\api_resources\\chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\openai\\api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    207\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    215\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    216\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    217\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    218\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    225\u001b[0m     )\n\u001b[1;32m--> 226\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\openai\\api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    612\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    613\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    614\u001b[0m         )\n\u001b[0;32m    615\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    616\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    618\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 619\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    620\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    621\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    622\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    623\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    624\u001b[0m         ),\n\u001b[0;32m    625\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    626\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\env\\lib\\site-packages\\openai\\api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    680\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    681\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 682\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    683\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Incorrect API key provided: sk-oWbZ7***************************************X6ta. You can find your API key at https://platform.openai.com/account/api-keys."
     ]
    }
   ],
   "source": [
    "llm  = ChatOpenAI(model_name=\"gpt-4-0613\", temperature=1)\n",
    "query = \"\"\"Example Update summary:\n",
    "Fully Custom Trained BERTopic Model for verification of disruption events from News Articles\n",
    "    * Webscraped 2.5k news articles from 4 different sources + manually labeled test set 375 \n",
    "    * Automated Grid search parameter tuning BERTopic model\n",
    "    * Evaluation BERTopic model\n",
    "Refactored code base\n",
    "    *Modify the schema of Supabase\n",
    "    *Bug-fix + Cleanup of technical debt\n",
    "    *End-to-End testing using pytest for key functionalities \n",
    "Learnt OpenAI API / Langchain\n",
    "    *Built custom-trained WhatsApp chatbot\n",
    "\n",
    "    \n",
    "Using that example, generate a summary of my work with the following details\n",
    "Points to anotate down\n",
    "HSC Everything\n",
    "Points \n",
    "Fully read 2 research papers ( Imitate their work wholesale first ) \n",
    "Visualize the Map \n",
    "Understand the Gurobi, \n",
    "Domain knowledge on \n",
    "\n",
    "Mini-task:\n",
    "Meeting to settle compatibility issue. First meeting with Liu Ning + Santosh. Since I've spent so long working out the compatibility, \n",
    "found the most promising method using torchscript c++ API to use ML model in c++ codebase.\n",
    "My task was, given a completed traning script to develop ML model in sklearn. Ensure the model was serialized in Pytorch format instead. \n",
    "c++ TorchScript Libtorch\n",
    "\n",
    "\n",
    "BERTopic Model improvements:\n",
    "Use entire News article content to train instead of just the title\n",
    "Re-trained BERTopic model with Both Title+Text data.\n",
    "- Created another Evaluation Dataset to test \n",
    "\n",
    "Evaulated performace:\n",
    "- Clusters Count for news article per topic became more distributed ( There's progress there )\n",
    "\n",
    "LLM:\n",
    "Custom trained my on personal data ( Can show demo )\n",
    "I did alot of Pinecone\n",
    "-Pinecone Query + OpenAI Text embedding\n",
    "- Cleaned and scraped 5.7k PDFs\n",
    "- Vectorized batched the pdfs into Pinecode DB\n",
    "\n",
    "Mirxes project:\n",
    "Got invited to join the actual Client Meeting with Mirxes ( Demo-ed the prototype to them ) \n",
    "Pivot-->Find how to make my output of mointoring disruption events to be useful\n",
    "\n",
    "Data Analyst on Mirxes internal data\n",
    "Show the mapping of the gastroclear graph\"\"\"\n",
    "\n",
    "result = llm.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_result = result.replace(\"emailAgent email:\", \"emailAgent email: \\n\")\n",
    "# Save the result to a text file\n",
    "with open(\"emailAgent.txt\", \"w\") as f:\n",
    "    f.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the singlife class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_description:  IndexDescription(name='singlife', metric='cosine', replicas=1, dimension=1536.0, shards=1, pods=1, pod_type='p1', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')\n",
      "index_stats_response:  {'dimension': 1536,\n",
      " 'index_fullness': 0.1,\n",
      " 'namespaces': {'': {'vector_count': 3488}},\n",
      " 'total_vector_count': 3488}\n",
      "vectorstore created succesfully\n",
      "Model successfully loaded: cache=None verbose=False callbacks=None callback_manager=None tags=None metadata=None client=<class 'openai.api_resources.chat_completion.ChatCompletion'> model_name='gpt-3.5-turbo-0613' temperature=0.0 model_kwargs={} openai_api_key='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' openai_api_base='' openai_organization='' openai_proxy='' request_timeout=None max_retries=6 streaming=False n=1 max_tokens=None tiktoken_model_name=None\n"
     ]
    }
   ],
   "source": [
    "# all the init is done in the Singlife class\n",
    "singlife = Singlife()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to initialize overall chain: -0.0009975433349609375\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGoal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \n",
      "                Custom knowledge base:Based on the provided context, it appears that the Singlife Corporate Travel Insurance policy is specifically designed for corporate travel purposes. It includes coverage for various aspects such as trip cancellation, medical and medical evacuation, personal liability, baggage loss or damage, and other travel-related incidents.\n",
      "\n",
      "However, since you are traveling for a ski trip with your family, it is important to note that the Singlife Corporate Travel Insurance may not provide specific coverage for leisure or personal trips. It is advisable to contact the insurance provider directly to confirm if the coverage extends to family trips and specifically covers skiing activities.\n",
      "\n",
      "If the Singlife Corporate Travel Insurance does not provide coverage for personal trips or skiing activities, you may need to consider purchasing a separate travel insurance policy that is specifically tailored for leisure travel and includes coverage for skiing-related incidents such as accidents or injuries on the slopes.\n",
      "\n",
      "It is recommended to thoroughly review the policy documents and consult with the insurance provider to ensure that you have the appropriate coverage for your family ski trip to Japan.\n",
      "\n",
      "Using the above information, generate a video script that addresses this user query:\n",
      "\n",
      "\"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\".\n",
      "Return the generated video script in the style/format: Funny and sarcastic\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'scene_assets': 'Scene: A family packing their bags for a ski trip. They are excited and energetic.',\n",
       " 'subtitle_script': \"Subtitle: So, you're going on a ski trip with your family, huh? Well, buckle up and get ready for some insurance talk!\",\n",
       " 'video_script': \"Scene: A family packing their bags for a ski trip. They are excited and energetic.\\n\\nSubtitle: So, you're going on a ski trip with your family, huh? Well, buckle up and get ready for some insurance talk!\\n\\nScene: Family members looking confused and worried.\\n\\nSubtitle: Now, here's the deal. The Singlife Corporate Travel Insurance policy may not be your best bet for this leisurely adventure.\\n\\nScene: Family members scratching their heads.\\n\\nSubtitle: It's designed for corporate travel, not family ski trips. Bummer, right?\\n\\nScene: Family members looking disappointed.\\n\\nSubtitle: But don't worry, we've got a solution for you!\\n\\nScene: Family members looking hopeful.\\n\\nSubtitle: You'll need to find a travel insurance policy that's specifically tailored for leisure travel and covers skiing activities.\\n\\nScene: Family members nodding in agreement.\\n\\nSubtitle: That way, you'll be protected from accidents, injuries, and all the fun stuff that can happen on the slopes.\\n\\nScene: Family members smiling and giving a thumbs up.\\n\\nSubtitle: So, before you hit the slopes, make sure to review the policy documents and consult with the insurance provider.\\n\\nScene: Family members holding up a phone and making a call.\\n\\nSubtitle: They'll give you all the details you need to ensure your family ski trip to Japan is covered.\\n\\nScene: Family members celebrating and cheering.\\n\\nSubtitle: Now, go have a blast on those snowy mountains! And remember, stay safe and insured!\\n\\nScene: Family members waving goodbye.\\n\\nSubtitle: Good luck and have an amazing ski trip!\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = singlife.generateScript(query=query, model_name=\"gpt-3.5-turbo-0613\", video_style=\"Funny and sarcastic\")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
