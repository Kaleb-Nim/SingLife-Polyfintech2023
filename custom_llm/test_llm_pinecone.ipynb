{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with promopts\n",
    "\n",
    "Assumed Pinecone db is already created and populated with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "import openai\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Langchain\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.callbacks import wandb_tracing_enabled\n",
    "from langchain.prompts import (\n",
    "    PromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from typing import Optional\n",
    "from langchain.chains import SimpleSequentialChain ,SequentialChain\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.schema import HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "# wandb\n",
    "import wandb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "* Load all the API keys from Pinecone and OpenAI\n",
    "* Load the Pinecone client\n",
    "* Set-up wandb tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables from the .env file\n",
    "load_dotenv('../Sn33k/.env')\n",
    "\n",
    "# Access the variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "INDEX_NAME = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "PINECONE_ENVIRONMENT= os.getenv(\"PINECONE_ENVIRONMENT\")\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "# wandb\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'test_llm_pinecone.ipynb'\n",
    "# here we are configuring the wandb project name\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Singlife\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WANDB setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaleb-nim\u001b[0m (\u001b[33mshiok\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb_config = {\"project\": \"Singlife\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Streaming LangChain activity to W&B at https://wandb.ai/shiok/Singlife/runs/7ftek8vp\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: `WandbTracer` is currently in beta.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Please report any issues to https://github.com/wandb/wandb/issues with the tag `langchain`.\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.tracers import WandbTracer \n",
    "Tracer = WandbTracer(wandb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],\n",
       " 'kwargs': {'model_name': 'gpt-3.5-turbo-0613',\n",
       "  'temperature': 0.0,\n",
       "  'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt-3.5-turbo-0613\"\n",
    "temperature = 0.0\n",
    "llm_qa = ChatOpenAI(model_name=model_name, temperature=temperature)\n",
    "llm_qa.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'chat_models', 'openai', 'ChatOpenAI'],\n",
       " 'kwargs': {'model_name': 'gpt-3.5-turbo-0613',\n",
       "  'temperature': 0.0,\n",
       "  'model_kwargs': {'functions': [{'name': 'output_formatter',\n",
       "     'description': 'Output formatter. Should always be used to format your response to the user.',\n",
       "     'parameters': {'title': 'generate_video_script',\n",
       "      'description': 'Generates 15-30sec video script based on custom knowledge base. Two components 1.Scene assets descriptions 2.Subtitle script',\n",
       "      'type': 'object',\n",
       "      'properties': {'list_of_video_chunk': {'type': 'array',\n",
       "        'description': 'List of video_chunk to be included in the video, one video chunk should last 3-5 seconds and includes: 1. Scene  2. Subtitle',\n",
       "        'items': {'type': 'object'},\n",
       "        'properties': {'scene': {'type': 'string',\n",
       "          'description': 'Scene description for video should be visual and general'},\n",
       "         'subtitles': {'type': 'string',\n",
       "          'description': 'Funny and sarcastic video subtitles script for video'}},\n",
       "        'required': ['scene', 'subtitles']}}}}],\n",
       "   'function_call': {'name': 'output_formatter'}},\n",
       "  'openai_api_key': {'lc': 1, 'type': 'secret', 'id': ['OPENAI_API_KEY']}}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"gpt-3.5-turbo-0613\"\n",
    "temperature = 0.0\n",
    "llm_video_script = ChatOpenAI(model_name=model_name, temperature=temperature, model_kwargs= {\"functions\":[\n",
    "    {\n",
    "      \"name\": \"output_formatter\",\n",
    "      \"description\": \"Output formatter. Should always be used to format your response to the user.\",\n",
    "      \"parameters\": {\n",
    "        \"title\": \"generate_video_script\",\n",
    "        \"description\": \"Generates 15-30sec video script based on custom knowledge base. Two components 1.Scene assets descriptions 2.Subtitle script\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"list_of_video_chunk\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of video_chunk to be included in the video, one video chunk should last 3-5 seconds and includes: 1. Scene  2. Subtitle\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\"\n",
    "            },\n",
    "            \"properties\": {\n",
    "              \"scene\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Scene description for video should be visual and general\"\n",
    "              },\n",
    "              \"subtitles\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Funny and sarcastic video subtitles script for video\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"scene\", \"subtitles\"]\n",
    "          }\n",
    "        },\n",
    "      }\n",
    "    }\n",
    "],\"function_call\":{\"name\":\"output_formatter\"}})\n",
    "llm_video_script.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorstore Setup ( Pinecone )\n",
    "\n",
    "Pinecone integration with Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Pinecone client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_description:  IndexDescription(name='singlife', metric='cosine', replicas=1, dimension=1536.0, shards=1, pods=1, pod_type='p1', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')\n",
      "index_stats_response:  {'dimension': 1536,\n",
      " 'index_fullness': 0.1,\n",
      " 'namespaces': {'': {'vector_count': 3488}},\n",
      " 'total_vector_count': 3488}\n"
     ]
    }
   ],
   "source": [
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_ENVIRONMENT,  # next to api key in console\n",
    ")\n",
    "\n",
    "index_name = \"singlife\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "# if you already have an index, you can load it like this\n",
    "docsearch = Pinecone.from_existing_index(index_name, embeddings)\n",
    "\n",
    "# List all indexes information\n",
    "index_description = pinecone.describe_index(index_name)\n",
    "print('index_description: ', index_description)\n",
    "\n",
    "index = pinecone.Index(index_name) \n",
    "index_stats_response = index.describe_index_stats()\n",
    "print('index_stats_response: ', index_stats_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Pinecone(index, embeddings.embed_query, \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling\n",
    "\n",
    "> describe functions to gpt-4-0613 and gpt-3.5-turbo-0613, and have the model intelligently choose to output a JSON object containing arguments to call those functions\n",
    "\n",
    "\n",
    "models have been fine-tuned to both detect when a function needs to be called (depending on the user’s input) and to respond with JSON that adheres to the function signature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions are specified with the following fields:\n",
    "\n",
    "* Name: The name of the function.\n",
    "* Description: A description of what the function does. The model will use this to decide when to call the function.\n",
    "* Parameters: The parameters object contains all of the input fields the function requires. These inputs can be of the following types: String, Number, Boolean, Object, Null, AnyOf. Refer to the API reference docs for details.\n",
    "* Required: Which of the parameters are required to make a query. The rest will be treated as optional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create QAretriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_qa,\n",
    "    chain_type=\"refine\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(memory=None, callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, combine_documents_chain=RefineDocumentsChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, input_key='input_documents', output_key='output_text', initial_llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['question', 'context_str'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context_str'], output_parser=None, partial_variables={}, template='Context information is below. \\n---------------------\\n{context_str}\\n---------------------\\nGiven the context information and not prior knowledge, answer any questions', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), refine_llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['question', 'context_str', 'existing_answer'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['existing_answer'], output_parser=None, partial_variables={}, template='{existing_answer}', template_format='f-string', validate_template=True), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context_str'], output_parser=None, partial_variables={}, template=\"We have the opportunity to refine the existing answer(only if needed) with some more context below.\\n------------\\n{context_str}\\n------------\\nGiven the new context, refine the original answer to better answer the question. If the context isn't useful, return the original answer.\", template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.0, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), document_variable_name='context_str', initial_response_name='existing_answer', document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), return_intermediate_steps=False), input_key='query', output_key='result', return_source_documents=False, retriever=VectorStoreRetriever(tags=None, metadata=None, vectorstore=<langchain.vectorstores.pinecone.Pinecone object at 0x00000169ED87B130>, search_type='similarity', search_kwargs={}))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?',\n",
       " 'result': \"Based on the provided context, it appears that the Singlife Corporate Travel Insurance policy is not suitable for a personal ski trip with your family. The policy is specifically designed for corporate travel purposes and may not provide the necessary coverage for leisure activities such as skiing.\\n\\nFor a ski trip with your family, it is recommended to consider a travel insurance policy that specifically covers recreational activities like skiing. Look for policies that include coverage for medical expenses related to skiing accidents, emergency medical evacuation, trip cancellation or interruption, and lost or damaged ski equipment.\\n\\nIt is important to review the policy details, including coverage limits, exclusions, and any specific requirements or conditions related to skiing activities. Consider exploring travel insurance options from providers that specialize in leisure or sports-related activities to ensure you have the appropriate coverage for your family's ski trip.\\n\\nRemember to disclose any pre-existing medical conditions and discuss with the insurance provider to ensure that you have the necessary coverage for your specific needs during the ski trip.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\"\n",
    "response = qa_chain(query)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template for Video chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['result', 'query'], output_parser=None, partial_variables={}, template='Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \\n    Custom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\nReturn the generated video script in the style/format: Funny and sarcastic', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template for video script generation\n",
    "video_prompt = PromptTemplate(\n",
    "    template=\"\"\"Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \n",
    "    Custom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\nReturn the generated video script in the style/format: Funny and sarcastic\"\"\",\n",
    "    input_variables= [\"result\", \"query\"]\n",
    ")\n",
    "video_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['result', 'query'], output_parser=None, partial_variables={}, template='Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \\n    Custom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\nReturn the generated video script in the style/format: Funny and sarcastic', template_format='f-string', validate_template=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_prompt_style =\"Funny and sarcastic\"\n",
    "video_prompt = PromptTemplate(\n",
    "    template=\"\"\"Goal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \n",
    "    Custom knowledge base:{result}\\n\\nUsing the above information, generate a video script that addresses this user query:\\n\\n\"{query}\".\\nReturn the generated video script in the style/format: \"\"\"+video_prompt_style,\n",
    "    input_variables= [\"result\", \"query\"]\n",
    ")\n",
    "video_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the video Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "        \"title\": \"generate_video_script\",\n",
    "        \"description\": \"Generates 15-30sec video script based on custom knowledge base. Two components 1.Scene descriptions 2.Subtitle script\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"list_of_video_chunk\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of video_chunk to be included in the video, one video chunk should last 3-5 seconds and is a dictionary with keys: 1. Scene  2. Subtitle\",\n",
    "            \"items\": {\n",
    "              \"type\": \"object\"\n",
    "            },\n",
    "            \"properties\": {\n",
    "              \"scene\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Scene description for video should be visual and general\"\n",
    "              },\n",
    "              \"subtitles\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Funny and sarcastic video subtitles script for video\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"scene\", \"subtitles\"]\n",
    "          }\n",
    "        },\"required\": [\"list_of_video_chunk\"],\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_chain2 = create_structured_output_chain(json_schema, llm_qa, video_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain for video script generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_chain = SequentialChain(chains=[qa_chain, video_chain2],input_variables=[\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:av81yds5) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-cosmos-6</strong> at: <a href='https://wandb.ai/shiok/Singlife/runs/av81yds5' target=\"_blank\">https://wandb.ai/shiok/Singlife/runs/av81yds5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230719_142852-av81yds5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:av81yds5). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\51703\\Documents\\GitHub\\SingLife-Polyfintech2023\\custom_llm\\wandb\\run-20230719_170314-ws23vjbj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shiok/Singlife/runs/ws23vjbj' target=\"_blank\">twilight-snowflake-7</a></strong> to <a href='https://wandb.ai/shiok/Singlife' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shiok/Singlife' target=\"_blank\">https://wandb.ai/shiok/Singlife</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shiok/Singlife/runs/ws23vjbj' target=\"_blank\">https://wandb.ai/shiok/Singlife/runs/ws23vjbj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/shiok/Singlife/runs/ws23vjbj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1a1952a7730>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"Singlife\", config=wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGoal:Generate 15-30sec video script based on custom knowledge base (Information below) and user query. Two components 1.Scene assets descriptions 2.Subtitle script \n",
      "    Custom knowledge base:Based on the provided context, it appears that the Singlife Corporate Travel Insurance policy is not suitable for a personal ski trip with your family. The policy is specifically designed for corporate travel purposes and may not provide the necessary coverage for leisure activities such as skiing.\n",
      "\n",
      "For your ski trip to Japan, it is recommended to seek out a travel insurance policy that specifically covers recreational activities like skiing. Look for a policy that includes coverage for medical expenses related to skiing accidents, emergency medical evacuation, trip cancellation or interruption, and loss or damage to ski equipment.\n",
      "\n",
      "It is important to carefully review the policy details, including coverage limits, exclusions, and any additional options or endorsements that may be available to enhance your coverage specifically for skiing activities. Consider exploring travel insurance options from providers that specialize in leisure or sports-related activities to ensure you have the appropriate coverage for your ski trip with your family.\n",
      "\n",
      "Remember to disclose any pre-existing medical conditions to the insurance provider and inquire about coverage for those conditions as well. Having the right travel insurance coverage will give you peace of mind and protect you and your family during your ski trip in Japan.\n",
      "\n",
      "Using the above information, generate a video script that addresses this user query:\n",
      "\n",
      "\"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\".\n",
      "Return the generated video script in the style/format: Funny and sarcastic\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'scene_assets_description': \"The video starts with a family packing their bags for their ski trip to Japan. They are excited and energetic, wearing winter clothes and holding ski equipment. The scene transitions to a comical animation of a corporate office, with people in suits and ties. The animation shows a sign that says 'Singlife Corporate Travel Insurance'. The scene quickly changes to a beautiful snowy mountain in Japan, with skiers enjoying their time on the slopes. The video ends with the family happily skiing together and a text overlay that says 'Get the right coverage for your ski trip!'\",\n",
       " 'subtitle_script': \"Subtitle 1: Family: We're going skiing in Japan!\\nSubtitle 2: Narrator: But wait, not all travel insurance policies are created equal.\\nSubtitle 3: Narrator: The Singlife Corporate Travel Insurance policy may not be suitable for your family ski trip.\\nSubtitle 4: Narrator: You need a policy that covers recreational activities like skiing.\\nSubtitle 5: Narrator: Look for coverage for medical expenses, emergency evacuation, trip cancellation, and ski equipment.\\nSubtitle 6: Narrator: Don't forget to review the policy details and exclusions.\\nSubtitle 7: Narrator: And hey, disclose any pre-existing medical conditions too!\\nSubtitle 8: Narrator: Get the right coverage for your ski trip and have a blast in Japan!\\nSubtitle 9: Text Overlay: Get the right coverage for your ski trip!\",\n",
       " 'video_length': '15-30 seconds'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = overall_chain.run(query=query,callbacks=[Tracer])\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the experiment begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I am travelling to Japan for a ski trip with my family next week.What kind of travel insurance coverage do we need?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ChatOpenAI' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[253], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39m# wandb trace\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mwith\u001b[39;00m wandb_tracing_enabled():\n\u001b[1;32m---> 17\u001b[0m     first_response \u001b[39m=\u001b[39m llm_video_script\u001b[39m.\u001b[39;49mrun(result\u001b[39m=\u001b[39muser_request, query\u001b[39m=\u001b[39mquery)\n\u001b[0;32m     19\u001b[0m first_response\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ChatOpenAI' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "user_request = \"\"\"Based on the Singlife Corporate Travel Insurance policy, you should consider the following coverage for your ski trip to Japan:\n",
    "1. Accidental death, permanent disablement and burns benefit: This provides coverage in case of accidental injury during your trip.\n",
    "2. Medical and medical evacuation: This is crucial for a ski trip as it covers any medical emergencies or injuries that may occur, including the cost of evacuation if necessary.\n",
    "3. Trip cancellation: If there's a last-minute cancellation, you can receive coverage for non-refundable deposits or unused travel and accommodation costs.\n",
    "4. Full terrorism cover: This offers a lump-sum payout if an unfortunate event occurs.\n",
    "5. Delayed departure, missed departure or connection: This covers any additional expenses incurred due to delayed or missed flights.\n",
    "6. Loss or damage of baggage and personal belongings: This provides coverage for lost or damaged personal items during your trip.\n",
    "7. Rental vehicle excess: If you plan to rent a vehicle during your trip, this coverage can be beneficial.\n",
    "8. COVID-19 Coverage: The policy covers trip interruptions or cancellations due to COVID-19 and covers the medical treatment if you contract COVID-19 during or after the trip.\n",
    "Remember to choose a plan that best suits your family's needs. The Elite Plan offers comprehensive coverage for frequent travellers, while the Classic Plan covers the basics.\n",
    "\n",
    "Using the above information, generate a video script that addresses this user query:I am travelling to Japan for a ski \n",
    "trip with my family next week. What kind of travel insurance coverage do we need? In the format of sarcastic and funny.\"\"\"\n",
    "\n",
    "# wandb trace\n",
    "with wandb_tracing_enabled():\n",
    "    first_response = llm_video_script\n",
    "\n",
    "first_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation testing ( Delete kalate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.openai_functions.openapi import get_openapi_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SequentialChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, chains=[LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=ChatPromptTemplate(input_variables=['query'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], output_parser=None, partial_variables={}, template=\"Use the provided API's to respond to this user query:\\n\\n{query}\", template_format='f-string', validate_template=True), additional_kwargs={})]), llm=ChatOpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-0613', temperature=0.7, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', request_timeout=None, max_retries=6, streaming=False, n=1, max_tokens=None, tiktoken_model_name=None), output_key='function', output_parser=JsonOutputFunctionsParser(args_only=False), return_final_only=True, llm_kwargs={'functions': [{'name': 'productsUsingGET', 'description': 'API for fetching Klarna product information', 'parameters': {'type': 'object', 'properties': {'params': {'type': 'object', 'properties': {'countryCode': {'type': 'string', 'description': 'ISO 3166 country code with 2 characters based on the user location. Currently, only US, GB, DE, SE and DK are supported.'}, 'q': {'type': 'string', 'description': \"A precise query that matches one very small category or product that needs to be searched for to find the products the user is looking for. If the user explicitly stated what they want, use that as a query. The query is as specific as possible to the product name or category mentioned by the user in its singular form, and don't contain any clarifiers like latest, newest, cheapest, budget, premium, expensive or similar. The query is always taken from the latest topic, if there is a new topic a new query is started. If the user speaks another language than English, translate their request into English (example: translate fia med knuff to ludo board game)!\"}, 'size': {'type': 'integer', 'description': 'number of products returned'}, 'min_price': {'type': 'integer', 'description': \"(Optional) Minimum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\"}, 'max_price': {'type': 'integer', 'description': \"(Optional) Maximum price in local currency for the product searched for. Either explicitly stated by the user or implicitly inferred from a combination of the user's request and the kind of product searched for.\"}}, 'required': ['countryCode', 'q']}}}}]}), SimpleRequestChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, request_method=<function get_openapi_chain.<locals>.<lambda> at 0x000001A192683AC0>, output_key='response', input_key='function')], input_variables=['query'], output_variables=['response'], return_all=False)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = get_openapi_chain(\n",
    "    \"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\"\n",
    ")\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'products': [{'name': \"Levi's Men's Classic Button-Down Shirt, Medium\",\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3202043708/Clothing/Levi-s-Men-s-Classic-Button-Down-Shirt-Medium/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$15.99',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Blue,Purple,Black,Orange,Green',\n",
       "    'Size:S,XL,L,M,XXL']},\n",
       "  {'name': 'Cubavera Four Pocket Guayabera Shirt',\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3202055522/Clothing/Cubavera-Four-Pocket-Guayabera-Shirt/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$24.99',\n",
       "   'attributes': ['Material:Polyester,Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Red,White,Blue,Black',\n",
       "    'Properties:Pockets',\n",
       "    'Pattern:Solid Color',\n",
       "    'Size:S,XL,L,M,XXL']},\n",
       "  {'name': 'Short Sleeve Cotton Plaid Shirt',\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201940921/Clothing/Short-Sleeve-Cotton-Plaid-Shirt/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$16.99',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Gray,Blue,Green',\n",
       "    'Size:XL,L']},\n",
       "  {'name': 'Vineyard Vines Gingham On-The-Go brrr Classic Fit Shirt Crystal',\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201938510/Clothing/Vineyard-Vines-Gingham-On-The-Go-brrr-Classic-Fit-Shirt-Crystal/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$89.60',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Blue',\n",
       "    'Size:XL,XS,L,M']},\n",
       "  {'name': \"Carhartt Men's Loose Fit Midweight Short Sleeve Plaid Shirt\",\n",
       "   'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201826024/Clothing/Carhartt-Men-s-Loose-Fit-Midweight-Short-Sleeve-Plaid-Shirt/?utm_source=openai&ref-site=openai_plugin',\n",
       "   'price': '$17.49',\n",
       "   'attributes': ['Material:Cotton',\n",
       "    'Target Group:Man',\n",
       "    'Color:Red,Brown,Blue,Green',\n",
       "    'Properties:Pockets',\n",
       "    'Pattern:Checkered',\n",
       "    'Size:S,XL,L,M']}]}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What are some options for a men's large blue button down shirt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm  = ChatOpenAI(model_name=\"gpt-4-0613\", temperature=1)\n",
    "query = \"\"\"# Task \n",
    "emailAgent rephrases raw email in a professional, polite and concise manner. emailAgent rephrases email with a appropriate tone with suitable email etiqutte. emailAgent then gives comment about raw email and suggest area of improvement after providing the rephrased and modified email. Give suggestions in point form, while providing EXAMPLE from the inputted email. number of points is limited to 3. \n",
    "\n",
    "# Task modification\n",
    "During the conversation, user might want to fine tune the behaviour of emailAgent. The prompt will be identify and received by emailAgent by adding \"# Tune\" before a prompt. emailAgent will change the behaviour accordingly.\n",
    "\n",
    "# Procedure description \n",
    "emailAgent starts working by asking user to input \n",
    "original email\", then response with emailAgent email and emailAgent Suggestions under [emailAgent response].\n",
    "\n",
    "# Template\n",
    "//template start\n",
    "original email: //user inputted email\n",
    "[emailAgent response]\n",
    "emailAgent email:  //agent generate rephrased and modified version of email\n",
    "emailAgent Suggestions: //agent suggestion\n",
    "//template ends\n",
    "\n",
    "# Example \n",
    "original email:\n",
    "\n",
    "Hi Primus,\n",
    "\n",
    " I have attempted to use the home router, but unfortunately, I encountered some issues. Initially, I suspected that the problem was with the port. I then realized that the home router may take a longer time to restart.\n",
    "\n",
    "Additionlaly, I contacted the technician regarding the issue. they advised me that although the AirPort is currently functional, it may still be blocked in the future.  I don't have a the home router now so i'll visit on Friday.\n",
    "\n",
    "\n",
    "[emailAgent response]\n",
    "emailAgent email:\n",
    "Dear Primus,\n",
    "\n",
    "I am reaching out to you with regards to the issues I faced while attempting to use the home router. I wanted to inform you that despite my initial suspicion that the problem was with the port, I later realized that the home router may take a longer time to restart.\n",
    "\n",
    "In addition to this, I have also spoken with a technician who advised me that although the AirPort is currently functional, it may become blocked in the future. However, my mother has already taken the home router home with her. As such, I have decided to return to the hall on Sunday after my exam and visit again on Saturday.\n",
    "\n",
    "I appreciate your understanding and assistance in this matter.\n",
    "\n",
    "emailAgent Suggestions:\n",
    "Specify the exact issues faced while using the home router.\n",
    "Example: Instead of stating that you faced issues while using the home router, provide specific details on the nature of the issue, such as difficulty connecting to the internet or slow internet speeds.\n",
    "\n",
    "Use appropriate language and avoid using slang terms.\n",
    "Example: Instead of using the term \"mom,\" use \"my mother\" or \"my parent.\"\n",
    "\n",
    "\n",
    "original email:\n",
    "Hi Team,\n",
    "\n",
    "Hi Team,\n",
    "\n",
    "Attached is the excel that highlights the missing Vendor information for the following items: 21400171, 21400181, 21400191, 21400161\n",
    "\n",
    "Is it possible to fill in the Vendor information highlighted in Red brackets?\n",
    "\n",
    "Understand that during the meeting, it was mentioned that the above components aren’t actually Raw materials and are intermediates.\n",
    "If that’s the case, is it possible to obtain the raw materials/intermediates for these 4 components with the corresponding Vendor information? This to allow us to have a full mapping of the GC components.\n",
    "\n",
    "FYI: The excel file attached was made by concatenating the GC_BOM with the MasterData. \n",
    "\n",
    "Feel free to ask me if any clarification is needed.\n",
    "\n",
    "Best Regards,\n",
    "Kaleb\"\"\"\n",
    "\n",
    "result = llm.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_result = result.replace(\"emailAgent email:\", \"emailAgent email: \\n\")\n",
    "# Save the result to a text file\n",
    "with open(\"emailAgent.txt\", \"w\") as f:\n",
    "    f.write(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
